#chip
chip_rows: 7
chip_cols: 7
verbose: 1
enable_broken_patch: False

reward_scaling: True

rf_version: 3
num_qubits: 18
env_max_step: 10
#lstm
use_lstm: False
use_gae: True
#PPO

#GAE折扣因子 趋向于0时 低方差高偏差（适合确定性环境），趋向于1时 高方差低偏差（适合随机环境）
lambda_: 0.9
#熵奖励系数
entropy_coeff: 0.02
#价值函数损失权重
vf_loss_coeff: 0.5

#越大约束越强
kl_coeff: 0.2
kl_target: 0.08

#learning rate
gamma: 0.99
lr_grid: [5.0e-5]
gamma_grid: [0.99]
#fcnet
fcnet_activation: Swish
fcnet_hiddens: [1024, 1024]
head_fcnet_hiddens: [1024,512,256]
#cnn
conv_activation: Swish
enable_cnn: True



###Iters is the number of batches the model will train on and the number of times your model weights will be updated (not counting minibatches).
stop_iters:  3 # Default: default_iters

# If run --as-release-test, --as-test must also be set.
as_test: false
as_release_test: false

log_file_id: 0

#在config.py中有自动配置
num_gpus: none
#resume: False


#the save path of check_point zip file
check_point_zip_path: none

debug: False

explore_during_inference: False

#attention start
use_attention: False

# The number of transformer units within GTrXL.
# A transformer unit in GTrXL consists of :
# a) MultiHeadAttention module and
# b) a position-wise MLP.
attention_num_transformer_units: 8


# The number of attention heads within the MultiHeadAttention units.
attention_num_heads: 4
# The dim of a single head (within the MultiHeadAttention units).
attention_head_dim: 256

# Whether to feed a_{t-n:t-1} to GTrXL (one-hot encoded if discrete).
prev_n_actions: 0
# Whether to feed r_{t-n:t-1} to GTrXL.
prev_n_rewards: 0
 # The input and output size of each transformer unit.
attention_dim: 256

# The memory sizes for inference and training.
attention_memory_inference: 10
attention_memory_training: 10

# The output dim of the position-wise MLP.
attention_position_wise_mlp_dim: 256
# The initial bias values for the 2 GRU gates within a transformer unit.
attention_init_gru_gate_bias: 2.0

#attention end

#the reward for multi-agent is the total sum (not the mean) over the agents.
stop_reward: 100.0

#One call to envs.step() is one timestep.
stop_timesteps: 9999999  # Default: default_timesteps


#unimportant
plot_trace: False
plot_result: False
storage_path: ' '