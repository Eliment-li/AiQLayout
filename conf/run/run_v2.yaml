rf_version: 1
env_version: 0
num_qubits: 4
env_max_step: 4
#chip
chip_rows: 9
chip_cols: 9
verbose: 1
#lstm
use_lstm: True

#the reward for multi-agent is the total sum (not the mean) over the agents.
stop_reward: 100.0

#One call to envs.step() is one timestep.
stop_timesteps: 9999999  # Default: default_timesteps

###Iters is the number of batches the model will train on and the number of times your model weights will be updated (not counting minibatches).
stop_iters: 2  # Default: default_iters

# If run --as-release-test, --as-test must also be set.
as_test: false
as_release_test: false

#learning rate
lr: 3.0e-5
lr_grid: [3.0e-5]
gamma: 0.99
gamma_grid: [0.99]

fcnet_activation: [Swish]
fcnet_hiddens: [128,256,256,128]
fcnet_hiddens_grid: [[128,256,256,128]]

local_mode: False
checkpoint_frequency: 10
checkpoint_at_end: True

log_file_id: 0

#在config.py中有自动配置
num_gpus: none
#resume: False

checkpoint: none

#the save path of check_point zip file
check_point_zip_path: none

debug: False

explore_during_inference: False

#attention start
use_attention: False

# The number of transformer units within GTrXL.
# A transformer unit in GTrXL consists of :
# a) MultiHeadAttention module and
# b) a position-wise MLP.
attention_num_transformer_units: 8


# The number of attention heads within the MultiHeadAttention units.
attention_num_heads: 4
# The dim of a single head (within the MultiHeadAttention units).
attention_head_dim: 256

# Whether to feed a_{t-n:t-1} to GTrXL (one-hot encoded if discrete).
prev_n_actions: 0
# Whether to feed r_{t-n:t-1} to GTrXL.
prev_n_rewards: 0
 # The input and output size of each transformer unit.
attention_dim: 256

# The memory sizes for inference and training.
attention_memory_inference: 10
attention_memory_training: 10

# The output dim of the position-wise MLP.
attention_position_wise_mlp_dim: 256
# The initial bias values for the 2 GRU gates within a transformer unit.
attention_init_gru_gate_bias: 2.0

#attention end



#unimportant
plot_trace: False
plot_result: False
storage_path: ' '